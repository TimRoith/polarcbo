<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>scipy.special._logsumexp &mdash; polarcbo v0.1.1 documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/style.css" type="text/css" />
    <link rel="shortcut icon" href="../../../_static/polar.ico"/>
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            polarcbo
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../dynamic.html">dynamic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../noise.html">Noise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../objectives.html">Objectives</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../scheduler.html">Scheduler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../utils.html">Scheduler</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">polarcbo</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">scipy.special._logsumexp</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for scipy.special._logsumexp</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy._lib._util</span> <span class="kn">import</span> <span class="n">_asarray_validated</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;logsumexp&quot;</span><span class="p">,</span> <span class="s2">&quot;softmax&quot;</span><span class="p">,</span> <span class="s2">&quot;log_softmax&quot;</span><span class="p">]</span>


<div class="viewcode-block" id="logsumexp"><a class="viewcode-back" href="../../../utils.html#polarcbo.utils.logsumexp">[docs]</a><span class="k">def</span> <span class="nf">logsumexp</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_sign</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the log of the sum of exponentials of input elements.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    a : array_like</span>
<span class="sd">        Input array.</span>
<span class="sd">    axis : None or int or tuple of ints, optional</span>
<span class="sd">        Axis or axes over which the sum is taken. By default `axis` is None,</span>
<span class="sd">        and all elements are summed.</span>

<span class="sd">        .. versionadded:: 0.11.0</span>
<span class="sd">    b : array-like, optional</span>
<span class="sd">        Scaling factor for exp(`a`) must be of the same shape as `a` or</span>
<span class="sd">        broadcastable to `a`. These values may be negative in order to</span>
<span class="sd">        implement subtraction.</span>

<span class="sd">        .. versionadded:: 0.12.0</span>
<span class="sd">    keepdims : bool, optional</span>
<span class="sd">        If this is set to True, the axes which are reduced are left in the</span>
<span class="sd">        result as dimensions with size one. With this option, the result</span>
<span class="sd">        will broadcast correctly against the original array.</span>

<span class="sd">        .. versionadded:: 0.15.0</span>
<span class="sd">    return_sign : bool, optional</span>
<span class="sd">        If this is set to True, the result will be a pair containing sign</span>
<span class="sd">        information; if False, results that are negative will be returned</span>
<span class="sd">        as NaN. Default is False (no sign information).</span>

<span class="sd">        .. versionadded:: 0.16.0</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    res : ndarray</span>
<span class="sd">        The result, ``np.log(np.sum(np.exp(a)))`` calculated in a numerically</span>
<span class="sd">        more stable way. If `b` is given then ``np.log(np.sum(b*np.exp(a)))``</span>
<span class="sd">        is returned.</span>
<span class="sd">    sgn : ndarray</span>
<span class="sd">        If return_sign is True, this will be an array of floating-point</span>
<span class="sd">        numbers matching res and +1, 0, or -1 depending on the sign</span>
<span class="sd">        of the result. If False, only one result is returned.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    numpy.logaddexp, numpy.logaddexp2</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    NumPy has a logaddexp function which is very similar to `logsumexp`, but</span>
<span class="sd">    only handles two arguments. `logaddexp.reduce` is similar to this</span>
<span class="sd">    function, but may be less stable.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from scipy.special import logsumexp</span>
<span class="sd">    &gt;&gt;&gt; a = np.arange(10)</span>
<span class="sd">    &gt;&gt;&gt; logsumexp(a)</span>
<span class="sd">    9.4586297444267107</span>
<span class="sd">    &gt;&gt;&gt; np.log(np.sum(np.exp(a)))</span>
<span class="sd">    9.4586297444267107</span>

<span class="sd">    With weights</span>

<span class="sd">    &gt;&gt;&gt; a = np.arange(10)</span>
<span class="sd">    &gt;&gt;&gt; b = np.arange(10, 0, -1)</span>
<span class="sd">    &gt;&gt;&gt; logsumexp(a, b=b)</span>
<span class="sd">    9.9170178533034665</span>
<span class="sd">    &gt;&gt;&gt; np.log(np.sum(b*np.exp(a)))</span>
<span class="sd">    9.9170178533034647</span>

<span class="sd">    Returning a sign flag</span>

<span class="sd">    &gt;&gt;&gt; logsumexp([1,2],b=[1,-1],return_sign=True)</span>
<span class="sd">    (1.5413248546129181, -1.0)</span>

<span class="sd">    Notice that `logsumexp` does not directly support masked arrays. To use it</span>
<span class="sd">    on a masked array, convert the mask into zero weights:</span>

<span class="sd">    &gt;&gt;&gt; a = np.ma.array([np.log(2), 2, np.log(3)],</span>
<span class="sd">    ...                  mask=[False, True, False])</span>
<span class="sd">    &gt;&gt;&gt; b = (~a.mask).astype(int)</span>
<span class="sd">    &gt;&gt;&gt; logsumexp(a.data, b=b), np.log(5)</span>
<span class="sd">    1.6094379124341005, 1.6094379124341005</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">_asarray_validated</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">check_finite</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">broadcast_arrays</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">b</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="mf">0.</span>  <span class="c1"># promote to at least float</span>
            <span class="n">a</span><span class="p">[</span><span class="n">b</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>

    <span class="n">a_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">a_max</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">a_max</span><span class="p">[</span><span class="o">~</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">a_max</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">a_max</span><span class="p">):</span>
        <span class="n">a_max</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">b</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">a_max</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">a_max</span><span class="p">)</span>

    <span class="c1"># suppress warnings about log of zero</span>
    <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">errstate</span><span class="p">(</span><span class="n">divide</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">tmp</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">return_sign</span><span class="p">:</span>
            <span class="n">sgn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="n">s</span> <span class="o">*=</span> <span class="n">sgn</span>  <span class="c1"># /= makes more sense but we need zero -&gt; zero</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">keepdims</span><span class="p">:</span>
        <span class="n">a_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">a_max</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">+=</span> <span class="n">a_max</span>

    <span class="k">if</span> <span class="n">return_sign</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">sgn</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">out</span></div>


<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compute the softmax function.</span>

<span class="sd">    The softmax function transforms each element of a collection by</span>
<span class="sd">    computing the exponential of each element divided by the sum of the</span>
<span class="sd">    exponentials of all the elements. That is, if `x` is a one-dimensional</span>
<span class="sd">    numpy array::</span>

<span class="sd">        softmax(x) = np.exp(x)/sum(np.exp(x))</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : array_like</span>
<span class="sd">        Input array.</span>
<span class="sd">    axis : int or tuple of ints, optional</span>
<span class="sd">        Axis to compute values along. Default is None and softmax will be</span>
<span class="sd">        computed over the entire array `x`.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    s : ndarray</span>
<span class="sd">        An array the same shape as `x`. The result will sum to 1 along the</span>
<span class="sd">        specified axis.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The formula for the softmax function :math:`\sigma(x)` for a vector</span>
<span class="sd">    :math:`x = \{x_0, x_1, ..., x_{n-1}\}` is</span>

<span class="sd">    .. math:: \sigma(x)_j = \frac{e^{x_j}}{\sum_k e^{x_k}}</span>

<span class="sd">    The `softmax` function is the gradient of `logsumexp`.</span>

<span class="sd">    The implementation uses shifting to avoid overflow. See [1]_ for more</span>
<span class="sd">    details.</span>

<span class="sd">    .. versionadded:: 1.2.0</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] P. Blanchard, D.J. Higham, N.J. Higham, &quot;Accurately computing the</span>
<span class="sd">       log-sum-exp and softmax functions&quot;, IMA Journal of Numerical Analysis,</span>
<span class="sd">       Vol.41(4), :doi:`10.1093/imanum/draa038`.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from scipy.special import softmax</span>
<span class="sd">    &gt;&gt;&gt; np.set_printoptions(precision=5)</span>

<span class="sd">    &gt;&gt;&gt; x = np.array([[1, 0.5, 0.2, 3],</span>
<span class="sd">    ...               [1,  -1,   7, 3],</span>
<span class="sd">    ...               [2,  12,  13, 3]])</span>
<span class="sd">    ...</span>

<span class="sd">    Compute the softmax transformation over the entire array.</span>

<span class="sd">    &gt;&gt;&gt; m = softmax(x)</span>
<span class="sd">    &gt;&gt;&gt; m</span>
<span class="sd">    array([[  4.48309e-06,   2.71913e-06,   2.01438e-06,   3.31258e-05],</span>
<span class="sd">           [  4.48309e-06,   6.06720e-07,   1.80861e-03,   3.31258e-05],</span>
<span class="sd">           [  1.21863e-05,   2.68421e-01,   7.29644e-01,   3.31258e-05]])</span>

<span class="sd">    &gt;&gt;&gt; m.sum()</span>
<span class="sd">    1.0</span>

<span class="sd">    Compute the softmax transformation along the first axis (i.e., the</span>
<span class="sd">    columns).</span>

<span class="sd">    &gt;&gt;&gt; m = softmax(x, axis=0)</span>

<span class="sd">    &gt;&gt;&gt; m</span>
<span class="sd">    array([[  2.11942e-01,   1.01300e-05,   2.75394e-06,   3.33333e-01],</span>
<span class="sd">           [  2.11942e-01,   2.26030e-06,   2.47262e-03,   3.33333e-01],</span>
<span class="sd">           [  5.76117e-01,   9.99988e-01,   9.97525e-01,   3.33333e-01]])</span>

<span class="sd">    &gt;&gt;&gt; m.sum(axis=0)</span>
<span class="sd">    array([ 1.,  1.,  1.,  1.])</span>

<span class="sd">    Compute the softmax transformation along the second axis (i.e., the rows).</span>

<span class="sd">    &gt;&gt;&gt; m = softmax(x, axis=1)</span>
<span class="sd">    &gt;&gt;&gt; m</span>
<span class="sd">    array([[  1.05877e-01,   6.42177e-02,   4.75736e-02,   7.82332e-01],</span>
<span class="sd">           [  2.42746e-03,   3.28521e-04,   9.79307e-01,   1.79366e-02],</span>
<span class="sd">           [  1.22094e-05,   2.68929e-01,   7.31025e-01,   3.31885e-05]])</span>

<span class="sd">    &gt;&gt;&gt; m.sum(axis=1)</span>
<span class="sd">    array([ 1.,  1.,  1.])</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">_asarray_validated</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">check_finite</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">x_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">exp_x_shifted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_max</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">exp_x_shifted</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_x_shifted</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compute the logarithm of the softmax function.</span>

<span class="sd">    In principle::</span>

<span class="sd">        log_softmax(x) = log(softmax(x))</span>

<span class="sd">    but using a more accurate implementation.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : array_like</span>
<span class="sd">        Input array.</span>
<span class="sd">    axis : int or tuple of ints, optional</span>
<span class="sd">        Axis to compute values along. Default is None and softmax will be</span>
<span class="sd">        computed over the entire array `x`.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    s : ndarray or scalar</span>
<span class="sd">        An array with the same shape as `x`. Exponential of the result will</span>
<span class="sd">        sum to 1 along the specified axis. If `x` is a scalar, a scalar is</span>
<span class="sd">        returned.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    `log_softmax` is more accurate than ``np.log(softmax(x))`` with inputs that</span>
<span class="sd">    make `softmax` saturate (see examples below).</span>

<span class="sd">    .. versionadded:: 1.5.0</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from scipy.special import log_softmax</span>
<span class="sd">    &gt;&gt;&gt; from scipy.special import softmax</span>
<span class="sd">    &gt;&gt;&gt; np.set_printoptions(precision=5)</span>

<span class="sd">    &gt;&gt;&gt; x = np.array([1000.0, 1.0])</span>

<span class="sd">    &gt;&gt;&gt; y = log_softmax(x)</span>
<span class="sd">    &gt;&gt;&gt; y</span>
<span class="sd">    array([   0., -999.])</span>

<span class="sd">    &gt;&gt;&gt; with np.errstate(divide=&#39;ignore&#39;):</span>
<span class="sd">    ...   y = np.log(softmax(x))</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; y</span>
<span class="sd">    array([  0., -inf])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">_asarray_validated</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">check_finite</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">x_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">x_max</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">x_max</span><span class="p">[</span><span class="o">~</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">x_max</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">x_max</span><span class="p">):</span>
        <span class="n">x_max</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">tmp</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">x_max</span>
    <span class="n">exp_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span>

    <span class="c1"># suppress warnings about log of zero</span>
    <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">errstate</span><span class="p">(</span><span class="n">divide</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_tmp</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">tmp</span> <span class="o">-</span> <span class="n">out</span>
    <span class="k">return</span> <span class="n">out</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Leon Bungert, Tim Roith, Philipp Wacker.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>